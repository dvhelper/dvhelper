# -*- coding: utf-8 -*-
#region Imports
# æ ‡å‡†åº“å¯¼å…¥
import os
import sys
import time
import json
import re
import argparse
from pathlib import Path
import urllib.parse
from datetime import datetime, timedelta
from dataclasses import dataclass, field

# XMLå¤„ç†ç›¸å…³å¯¼å…¥
from xml.dom import minidom
import xml.etree.ElementTree as ET

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ï¼Œå…¶å®ƒç¬¬ä¸‰æ–¹åº“ç”±lazy_import()å¯¼å…¥
from rich_argparse import RawTextRichHelpFormatter
#endregion


#region Setup Console
sys.stdout.reconfigure(encoding='utf-8', errors='replace')
sys.stderr.reconfigure(encoding='utf-8', errors='replace')
#endregion


__version__ = '0.0.7'
__version_info__ = tuple(int(x) for x in __version__.split('.'))


@dataclass
class Config:
	# URLs
	base_url:    str = 'https://avfan.com'
	sign_in_url: str = f'{base_url}/zh-CN/sign_in'
	search_url:  str = f'{base_url}/search?q='

	#region File & Path names
	fanart_image:        str = 'fanart.jpg'
	poster_image:        str = 'poster.jpg'
	cookies_file:       Path = Path(__file__).parent.joinpath('cookies.json')
	actress_alias_file: Path = Path(__file__).parent.joinpath('actress_alias.json')
	completed_path:      str = '#æ•´ç†å®Œæˆ#'
	ignored_file_prefix: str = '##'
	exclude_path: tuple[str] = (
		completed_path,
	)
	#endregion

	#region Regex pattern
	ignored_keyword_pattern: list[str] = (
		r'(144|240|360|480|720|1080)[Pp]',
		r'[24][Kk]',
		r'\w+2048\.com',
		r'Carib(beancom)?',
		r'[^a-z\d](f?hd|lt)[^a-z\d]',
	)
	ignored_movie_pattern:  re.Pattern = re.compile('|'.join(ignored_keyword_pattern))
	normal_movie_pattern:   re.Pattern = re.compile(r'([A-Z]{2,10})[-_](\d{2,5})', re.I)
	normal_movie_pattern2:  re.Pattern = re.compile(r'([A-Z]{2,})(\d{2,5})', re.I)
	fc2_movie_pattern:      re.Pattern = re.compile(r'FC2[^A-Z\d]{0,5}(PPV[^A-Z\d]{0,5})?(\d{5,7})', re.I)
	_259luxu_movie_pattern: re.Pattern = re.compile(r'259LUXU-(\d+)', re.I)
	_200gana_movie_pattern: re.Pattern = re.compile(r'200GANA-(\d+)', re.I)
	_300mium_movie_pattern: re.Pattern = re.compile(r'300MIUM-(\d+)', re.I)
	#endregion

	# File extensions
	movie_file_extensions: tuple[str] = (
		'.3gp', '.avi', '.f4v', '.flv', '.iso', '.m2ts',
		'.m4v', '.mkv', '.mov', '.mp4', '.mpeg', '.rm',
		'.rmvb', '.ts', '.vob', '.webm', '.wmv', '.strm',
		'.mpg',
	)

	# CSS selectors
	search_target_class: str = 'flex flex-col relative hover:bg-zinc-100 hover:dark:bg-zinc-800'
	movie_target_class:  str = 'flex flex-col gap-2'

	# Actress name map
	actress_alias: dict[str, list[str]] = field(default_factory=dict)

	#region argparse help messages
	description:   str = f'[b]DV Helper (version [i]{__version__}[/]) - å½±ç‰‡ä¿¡æ¯æœç´¢å’ŒNFOç”Ÿæˆå·¥å…·\n\n  è‡ªåŠ¨æœç´¢å½±ç‰‡ä¿¡æ¯ï¼Œä¸‹è½½å°é¢å›¾ç‰‡ã€å‰§ç…§ã€é¢„å‘Šç‰‡ï¼Œç”ŸæˆNFOæ–‡ä»¶ï¼Œ\n  å¹¶æŒ‰æ¼”å‘˜åˆ†ç±»æ•´ç†å½±ç‰‡ï¼Œæ”¯æŒåœ¨çº¿æœç´¢å½±ç‰‡ä¿¡æ¯å’Œæ‰¹é‡å¤„ç†æœ¬åœ°å½±ç‰‡ç›®å½•ã€‚[/]'
	keywords_help: str = 'æœç´¢å…³é”®è¯ï¼ˆå¦‚å½±ç‰‡ç¼–å·ï¼‰æˆ–æœ¬åœ°å½±ç‰‡ç›®å½•è·¯å¾„\nå¯ä»¥ä½¿ç”¨é€—å·åˆ†éš”å¤šä¸ªå…³é”®è¯ï¼Œæˆ–æŒ‡å®šä¸€ä¸ªåŒ…å«å½±ç‰‡æ–‡ä»¶çš„ç›®å½•è¿›è¡Œæ‰¹é‡å¤„ç†'
	depth_help:    str = 'ç›®å½•æœç´¢æ·±åº¦ï¼ˆé»˜è®¤ï¼š%(default)sï¼Œè¡¨ç¤ºä»…æœç´¢å½“å‰ç›®å½•ï¼‰'
	login_help:    str = 'å¿½ç•¥å·²ä¿å­˜çš„ Cookie å¼ºåˆ¶è¿›è¡Œæ–°çš„ç™»å½•æ“ä½œ'
	gallery_help:  str = 'ä¸‹è½½å½±ç‰‡çš„å‰§ç…§å’Œé¢„å‘Šç‰‡'
	epilog:        str = '''
[argparse.groups]Examples:[/]
  [b]æœç´¢å½±ç‰‡ç¼–å·[/]
    [argparse.prog]%(prog)s[/] [argparse.args]ABCDE-123[/]

    æœç´¢ç¼–å·ä¸º [argparse.args]ABCDE-123[/] çš„å½±ç‰‡ä¿¡æ¯å¹¶åœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆæ•´ç†å¥½çš„å½±ç‰‡ç›®å½•
    å¯ä»¥ä½¿ç”¨é€—å·åˆ†éš”å¤šä¸ªæœç´¢å…³é”®è¯

  [b]æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„å½±ç‰‡æ–‡ä»¶[/]
    [argparse.prog]%(prog)s[/] [argparse.args]/path/to/movies[/] -d [argparse.metavar]1[/]

    æ‰«ææŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ä¸­çš„å½±ç‰‡æ–‡ä»¶å¹¶ç”Ÿæˆæ•´ç†å¥½çš„å½±ç‰‡ç›®å½•
    ä½¿ç”¨ -d å‚æ•°å¯ä»¥æŒ‡å®šå­ç›®å½•çš„æ‰«ææ·±åº¦ï¼Œå¦åˆ™ä»…æ‰«æå½“å‰ç›®å½•

  [b]å¼ºåˆ¶é‡æ–°ç™»å½•[/]
    [argparse.prog]%(prog)s[/] [argparse.args]ABCDE-123[/] -l

    å¼ºåˆ¶é‡æ–°ç™»å½•ï¼Œå¿½ç•¥å·²ä¿å­˜çš„ Cookie å¹¶è¿›è¡Œæ–°çš„ç™»å½•æ“ä½œ

  [b]æ•´ç†å¹¶é‡å‘½åå½±ç‰‡æ–‡ä»¶å¤¹[/]
    [argparse.prog]%(prog)s[/] [argparse.args]/path/to/movies[/] -o

    æ•´ç†å¹¶é‡å‘½åæŒ‡å®šç›®å½•ä¸‹çš„å½±ç‰‡æ–‡ä»¶å¤¹ï¼Œä¸ä¼šå®é™…æ‰§è¡Œæ–‡ä»¶æ“ä½œ
    è¯¥åŠŸèƒ½ä¼šæ ¹æ®actress_alias.jsonä¸­çš„æ˜ å°„è¡¨é€’å½’æŸ¥æ‰¾å¹¶è¯†åˆ«éœ€è¦é‡å‘½åçš„æ–‡ä»¶å¤¹
'''
	#endregion


#region Base Classes
class TqdmOut:
	"""ç”¨äºå°†loggingçš„streamè¾“å‡ºé‡å®šå‘åˆ°tqdm"""
	@classmethod
	def write(cls, s, file=None, nolock=False):
		tqdm.write(s, file=file, end='', nolock=nolock)


class HelpOnErrorParser(argparse.ArgumentParser):
	def error(self, message):
		sys.stderr.write(f'é”™è¯¯: {message}\n')
		self.print_help()
		sys.exit(2)


class MovieInfo():
	"""å½±ç‰‡ä¿¡æ¯æ•°æ®ç±»ï¼Œç»Ÿä¸€ç®¡ç†å½±ç‰‡ç›¸å…³ä¿¡æ¯
	
	Attributes:
		detail_url : å½±ç‰‡è¯¦æƒ…é¡µURL
		fanart_url : å°é¢å›¾ç‰‡URL
		number     : å½±ç‰‡ç¼–å·
		trailer_url: é¢„å‘Šç‰‡URL
		galleries  : å‰§ç…§URLåˆ—è¡¨
		title      : å½±ç‰‡æ ‡é¢˜
		year       : å‘è¡Œå¹´ä»½
		runtime    : å½±ç‰‡æ—¶é•¿(åˆ†é’Ÿ)
		tags       : æ ‡ç­¾åˆ—è¡¨
		actresses  : å¥³æ¼”å‘˜åˆ—è¡¨
		director   : å¯¼æ¼”
		studio     : åˆ¶ä½œå•†
		publisher  : å‘è¡Œå•†
		premiered  : å‘è¡Œæ—¥æœŸ
		mpaa       : åˆ†çº§
		country    : å›½å®¶/åœ°åŒº
	"""

	def __init__(self, info: dict):
		self.info = info
		self.detail_url:      str = info.get('detail_url', '')
		self.fanart_url:      str = info.get('fanart_url', '')
		self.trailer_url:     str = info.get('trailer_url', '')
		self.galleries: list[str] = info.get('galleries', [])
		self.number:          str = info.get('number', '')
		self.title:           str = info.get('title', '')
		self.year:            str = info.get('year', '')
		self.runtime:         str = info.get('runtime', '')
		self.tags:      list[str] = info.get('tags', [])
		self.actresses: list[str] = info.get('actresses', [])
		self.director:        str = info.get('director', '')
		self.studio:          str = info.get('studio', '')
		self.publisher:       str = info.get('publisher', '')
		self.premiered:       str = info.get('premiered', '')
		self.mpaa:            str = info.get('mpaa', 'NC-17')
		self.country:         str = info.get('country', 'æ—¥æœ¬')


class NFOGenerator():
	"""NFOæ–‡ä»¶ç”Ÿæˆå™¨ï¼Œå°†å½±ç‰‡ä¿¡æ¯è½¬æ¢ä¸ºé€šç”¨çš„NFOæ ¼å¼"""

	def __init__(self, movie_info: MovieInfo):
		self.root = ET.Element('movie')
		self.__add_movie_info(movie_info)

	def __add_movie_info(self, movie_info: MovieInfo):
		"""å°†å½±ç‰‡ä¿¡æ¯æ·»åŠ åˆ°XMLç»“æ„ä¸­

		Args:
			movie_info: åŒ…å«å½±ç‰‡ä¿¡æ¯çš„MovieInfoå¯¹è±¡
		"""
		# åŸºæœ¬ä¿¡æ¯
		ET.SubElement(self.root, 'title').text   = movie_info.title
		ET.SubElement(self.root, 'year').text    = movie_info.year
		ET.SubElement(self.root, 'runtime').text = movie_info.runtime
		ET.SubElement(self.root, 'mpaa').text    = movie_info.mpaa

		# å½±ç‰‡ç¼–å·ä½œä¸ºå”¯ä¸€æ ‡è¯†
		if movie_info.number:
			uniqueid        = ET.SubElement(self.root, 'uniqueid')
			uniqueid.text   = movie_info.number
			uniqueid.attrib = {'type': 'num', 'default': 'true'}

		# åˆ†ç±»å’Œæ ‡ç­¾
		if movie_info.tags:
			for genre in movie_info.tags:
				ET.SubElement(self.root, 'genre').text = genre
			for tag in movie_info.tags:
				ET.SubElement(self.root, 'tag').text   = tag

		# å…¶ä»–å…ƒæ•°æ®
		ET.SubElement(self.root, 'country').text = movie_info.country

		if movie_info.director:
			ET.SubElement(self.root, 'director').text  = movie_info.director
		if movie_info.premiered:
			ET.SubElement(self.root, 'premiered').text = movie_info.premiered
		if movie_info.studio:
			ET.SubElement(self.root, 'studio').text    = movie_info.studio
		if movie_info.publisher:
			ET.SubElement(self.root, 'publisher').text = movie_info.publisher

		# å¥³æ¼”å‘˜ä¿¡æ¯
		if movie_info.actresses:
			for actress_info in movie_info.actresses:
				actress = ET.SubElement(self.root, 'actress')
				ET.SubElement(actress, 'name').text = actress_info

		# åª’ä½“ä¿¡æ¯
		if movie_info.fanart_url:
			fanart = ET.SubElement(self.root, 'fanart')
			ET.SubElement(fanart, 'thumb').text      = movie_info.fanart_url
		if movie_info.trailer_url:
			ET.SubElement(self.root, 'trailer').text = movie_info.trailer_url

	def save(self, output_path: Path):
		"""å°†XMLç»“æ„ä¿å­˜ä¸ºæ ¼å¼åŒ–çš„NFOæ–‡ä»¶

		Args:
			output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
		"""
		# ç”Ÿæˆæ ¼å¼åŒ–çš„XMLå­—ç¬¦ä¸²
		rough_string = ET.tostring(self.root, 'utf-8')
		reparsed = minidom.parseString(rough_string)
		pretty_xml = reparsed.toprettyxml(
			indent='    ', encoding='utf-8', standalone=True
		).replace(b'&amp;', b'&')

		with open(output_path, 'wb') as f:
			f.write(pretty_xml)


class MovieParser():
	"""å½±ç‰‡ä¿¡æ¯è§£æå™¨ï¼Œä»HTMLå†…å®¹æå–å½±ç‰‡ç›¸å…³ä¿¡æ¯"""

	@staticmethod
	def parse_search_results(html: str, keyword: str):
		"""è§£ææœç´¢ç»“æœé¡µé¢ï¼Œæå–åŒ¹é…çš„å½±ç‰‡ä¿¡æ¯

		Args:
			html: æœç´¢ç»“æœé¡µé¢HTMLå†…å®¹
			keyword: æœç´¢å…³é”®è¯

		Returns:
			åŒ…å«å½±ç‰‡URLã€æ ‡é¢˜å’Œå°é¢å›¾ç‰‡URLçš„å­—å…¸ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›None
		"""
		if not html:
			return

		from bs4 import BeautifulSoup

		soup = BeautifulSoup(html, 'html.parser')
		elements = soup.find_all(class_=config.search_target_class)

		for element in elements:
			a_tag = element.find('a')

			if not a_tag:
				continue

			href: str    = a_tag.get('href', '')
			title: str   = a_tag.get('title', '').strip()
			img_tag      = a_tag.find('img')
			img_src: str = img_tag.get('src', '')

			if keyword.lower() in title.lower():
				return {
					'detail_url': f'{config.base_url}{href}',
					'title'     : title,
					'fanart_url': img_src
				}
		return

	@staticmethod
	def parse_movie_details(html: str):
		"""è§£æå½±ç‰‡è¯¦æƒ…é¡µé¢ï¼Œæå–è¯¦ç»†ä¿¡æ¯

		Args:
			html: å½±ç‰‡è¯¦æƒ…é¡µHTMLå†…å®¹

		Returns:
			åŒ…å«å½±ç‰‡è¯¦ç»†ä¿¡æ¯çš„å­—å…¸ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—å…¸
		"""
		results = {}

		if not html:
			return results

		from bs4 import BeautifulSoup

		soup = BeautifulSoup(html, 'html.parser')
		ul_elements = soup.find_all('ul', class_=config.movie_target_class)

		# æå–å½±ç‰‡è¯¦æƒ…
		for ul_element in ul_elements:
			li_elements = ul_element.find_all('li')

			if li_elements:
				li_contents = []
				for li in li_elements:
					for male_a in li.find_all('a', class_='male'):
						male_a.extract()
					li_contents.append(li.get_text(strip=True))
				results = MovieParser.__extract_info_from_list(li_contents)

		results['galleries'] = []
		a_elements = soup.find_all('a', {'data-fancybox': 'gallery'})

		# æå–é¢„å‘Šç‰‡å’Œå‰§ç…§
		for a_tag in a_elements:
			href: str = a_tag.get('href', '')
			data_caption: str = a_tag.get('data-caption', '').strip()

			if data_caption == 'é¢„å‘Šç‰‡':
				results['trailer_url'] = href
			else:
				results['galleries'].append(href)

		return results

	@staticmethod
	def __extract_info_from_list(content_list: list[str]):
		"""ä»åˆ—è¡¨å†…å®¹æå–å½±ç‰‡ä¿¡æ¯

		Args:
			content_list: åŒ…å«å½±ç‰‡ä¿¡æ¯çš„å­—ç¬¦ä¸²åˆ—è¡¨

		Returns:
			æå–çš„å½±ç‰‡ä¿¡æ¯å­—å…¸
		"""
		result = {}

		for item in content_list:
			if item.startswith('ç•ªå·:'):
				result['number'] = item.replace('ç•ªå·:', '').replace('å¤åˆ¶', '').strip()
			elif item.startswith('å‘è¡Œæ—¥æœŸ:'):
				result['premiered'] = item.replace('å‘è¡Œæ—¥æœŸ:', '').strip()
				if len(result['premiered']) >= 4:
					result['year'] = result['premiered'][:4]
			elif item.startswith('ç‰‡é•¿:'):
				result['runtime'] = item.replace('ç‰‡é•¿:', '').replace(' åˆ†é’Ÿ', '').strip()
			elif item.startswith('å¯¼æ¼”:'):
				result['director'] = item.replace('å¯¼æ¼”:', '').strip()
			elif item.startswith('åˆ¶ä½œå•†:'):
				result['studio'] = item.replace('åˆ¶ä½œå•†:', '').strip()
			elif item.startswith('å‘è¡Œå•†:'):
				result['publisher'] = item.replace('å‘è¡Œå•†:', '').strip()
			elif item.startswith('æ ‡ç­¾:'):
				result['tags'] = [tag.strip() for tag in item.replace('æ ‡ç­¾:', '').replace('--', '').split(',') if tag.strip()]
			elif item.startswith('æ¼”å‘˜:'):
				result['actresses'] = [actress.strip() for actress in item.replace('æ¼”å‘˜:', '').replace('--', '').split(',') if actress.strip()]

				if len(config.actress_alias):
					result['actresses'] = [MovieParser.__resolve_actress_alias(actress) for actress in result['actresses']]

		return result

	@staticmethod
	# https://github.com/Yuukiy/JavSP/blob/master/javsp/__main__.py#L53
	def __resolve_actress_alias(name: str):
		"""å°†åˆ«åè§£æä¸ºå›ºå®šçš„åå­—"""
		for fixed_name, aliases in config.actress_alias.items():
			if name in aliases:
				return fixed_name

		return name


class MovieScraper():
	"""å½±ç‰‡ä¿¡æ¯æŠ“å–å™¨ï¼Œå®ç°ç™»å½•ç®¡ç†ã€æ•°æ®å’Œå›¾ç‰‡çš„æŠ“å–æµç¨‹"""

	REQUESTS_HEADERS = {
		'Accept-Language': 'zh-CN,zh;q=0.9',
		'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
	}

	def __init__(self):
		self.__session = None

	def initialize_session(self):
		self.__session = self.check_cookies()

		if not self.__session:
			logger.warning('ğŸš« æœªæ‰¾åˆ°æœ‰æ•ˆCookiesï¼Œå°†ä½¿ç”¨åŒ¿åä¼šè¯ï¼Œæˆ–ä½¿ç”¨ -l å‚æ•°è¿›è¡Œç™»å½•æ“ä½œ')

	def check_cookies(self):
		"""æ£€æŸ¥å¹¶åŠ è½½Cookieï¼ŒéªŒè¯æœ‰æ•ˆæ€§

		Returns:
			æœ‰æ•ˆçš„requestsä¼šè¯å¯¹è±¡ï¼ŒCookiesè¿‡æœŸæˆ–ä¸å­˜åœ¨åˆ™è¿”å›None
		"""
		if not config.cookies_file.exists():
			logger.warning('ğŸš« Cookies æ–‡ä»¶ä¸å­˜åœ¨')
			return

		session = requests.Session()

		try:
			with open(config.cookies_file, 'r', encoding='utf-8') as f:
				cookies: list[dict] = json.load(f)

			for cookie in cookies:
				if 'expiry' in cookie and cookie['name'] == 'remember_token':
					expiry_time = datetime.fromtimestamp(cookie['expiry'])

					if expiry_time < datetime.now() - timedelta(seconds=60):
						logger.warning('ğŸš« å½“å‰ Cookie å·²è¿‡æœŸ')

						return

				session.cookies.set(
					cookie['name'],
					cookie['value'],
					domain=cookie.get('domain'),
					path=cookie.get('path', '/'),
					secure=cookie.get('secure', False)
				)

			return session
		except Exception as e:
			logger.error('ğŸš« Cookies æ–‡ä»¶å¤„ç†å¤±è´¥ï¼š%s', str(e))
			return

	def perform_login(self):
		"""ä½¿ç”¨Seleniumæ‰§è¡Œç™»å½•å¹¶ä¿å­˜Cookie

		Returns:
			ç™»å½•åçš„requestsä¼šè¯å¯¹è±¡ï¼Œç™»å½•å¤±è´¥åˆ™è¿”å›None
		"""
		from selenium import webdriver
		from selenium.webdriver.support.ui import WebDriverWait
		from selenium.webdriver.support import expected_conditions as EC
		from selenium.webdriver.chrome.options import Options
		from webdriver_manager.chrome import ChromeDriverManager
		from selenium.webdriver.chrome.service import Service

		# é…ç½®Chromeé€‰é¡¹
		chrome_options = Options()
		chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

		# ä½¿ç”¨webdriver-managerè‡ªåŠ¨ç®¡ç†ChromeDriver
		service = Service(ChromeDriverManager().install())
		driver = webdriver.Chrome(service=service, options=chrome_options)

		try:
			logger.info('ğŸ”„ æ­£åœ¨å¯åŠ¨ Chrome æµè§ˆå™¨...')

			print('åœ¨å¼¹å‡ºçš„ç½‘é¡µä¸­å®Œæˆç™»å½•æ“ä½œï¼Œç­‰å¾…æµè§ˆå™¨è‡ªåŠ¨å…³é—­ï¼\n'*3)
			driver.get(config.sign_in_url)

			# ç­‰å¾…ç”¨æˆ·å®Œæˆç™»å½•å¹¶é‡å®šå‘
			WebDriverWait(driver, 3 * 60).until(
				EC.url_to_be(f'{config.base_url}/')
			)

			# è·å–å¹¶ä¿å­˜Cookie
			cookies = driver.get_cookies()
			with open(config.cookies_file, 'w', encoding='utf-8') as f:
				json.dump(cookies, f, ensure_ascii=False, indent=2)

			logger.info('âœ… å·²ä¿å­˜ %d ä¸ª Cookie åˆ° %s', len(cookies), config.cookies_file)

			# åˆ›å»ºä¼šè¯å¹¶åŠ è½½Cookie
			session = requests.Session()

			for cookie in cookies:
				session.cookies.set(
					cookie['name'],
					cookie['value'],
					domain=cookie.get('domain'),
					path=cookie.get('path', '/'),
					secure=cookie.get('secure', False)
				)
		except Exception:
			session = None
			logger.error('ğŸš« ç”¨æˆ·ç™»å½•å¤±è´¥')
		finally:
			time.sleep(2)
			driver.quit()

		return session

	def fetch_data(self, url: str, max_retries: int=3, initial_timeout: int=30, backoff_factor: int=2):
		"""è·å–æŒ‡å®šç½‘ç«™çš„æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒé‡è¯•æ“ä½œ

		Args:
			url: ç›®æ ‡ç½‘å€
			max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
			initial_timeout: åˆå§‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
			backoff_factor: é€€é¿å› å­ï¼Œé»˜è®¤2

		Returns:
			å“åº”å†…å®¹æ–‡æœ¬ï¼Œæ‰€æœ‰é‡è¯•å¤±è´¥åˆ™è¿”å›None
		"""
		for retry in range(1, max_retries + 1):
			current_timeout = initial_timeout * (backoff_factor ** (retry - 1))

			if retry > max_retries:
				print(f'ç¬¬ {retry}/{max_retries} æ¬¡å°è¯•ï¼ˆè¶…æ—¶æ—¶é—´ï¼š{current_timeout} ç§’ï¼‰')

			try:
				if self.__session:
					response =  self.__session.get(url=url, headers=self.REQUESTS_HEADERS, timeout=current_timeout)
				else:
					response = requests.get(url=url, headers=self.REQUESTS_HEADERS, timeout=current_timeout)

				response.encoding = 'utf-8' # response.apparent_encoding
				response.raise_for_status()

				return response.text
			except (RequestException, Timeout):
				if retry >= max_retries:
					return

	def fetch_media(self, movie_path: Path, media_file: str, url: str, crop: bool=False, max_retries=3, initial_timeout=30, backoff_factor=2):
		"""ä¸‹è½½å½±ç‰‡ç›¸å…³åª’ä½“æ–‡ä»¶ï¼ˆåŒ…å«å½±ç‰‡å°é¢å›¾ç‰‡ã€å‰§ç…§ã€é¢„å‘Šç‰‡ç­‰ï¼‰

		ä»æŒ‡å®šåœ°å€ä¸‹è½½å½±ç‰‡åª’ä½“æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ï¼Œå¹¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦

		Args:
			movie_path: åª’ä½“æ–‡ä»¶ä¿å­˜è·¯å¾„
			media_file: åª’ä½“æ–‡ä»¶å
			url: åª’ä½“æ–‡ä»¶ä¸‹è½½åœ°å€
			crop: æ˜¯å¦è£å‰ªå›¾ç‰‡ï¼Œé»˜è®¤False
			max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
			initial_timeout: åˆå§‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
			backoff_factor: é€€é¿å› å­ï¼Œç”¨äºæŒ‡æ•°é€€é¿ç®—æ³•ï¼Œé»˜è®¤2

		Returns:
			ä¸‹è½½å’Œè£å‰ªæˆåŠŸè¿”å›Trueï¼Œå¤±è´¥åˆ™è¿”å›False
		"""
		for retry in range(1, max_retries + 1):
			current_timeout = initial_timeout * (backoff_factor ** (retry - 1))

			if retry > max_retries:
				print(f'ç¬¬ {retry}/{max_retries} æ¬¡å°è¯•ï¼ˆè¶…æ—¶æ—¶é—´ï¼š{current_timeout} ç§’ï¼‰')

			try:
				response = requests.get(url, stream=True, timeout=current_timeout)
				response.raise_for_status()

				# è·å–æ–‡ä»¶å¤§å°
				total_size = int(response.headers.get('content-length', 0))
				chunk_size = 8192

				media_file = movie_path / media_file
				with open(media_file, 'wb') as f:
					with tqdm(total=total_size, unit='B', unit_scale=True, desc=f'åª’ä½“æ–‡ä»¶ï¼š{media_file.name}', leave=False, ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}') as pbar:
						for chunk in response.iter_content(chunk_size=chunk_size):
							if chunk:
								f.write(chunk)
								pbar.update(len(chunk))

				if crop:
					self.crop_image(media_file, movie_path / config.poster_image)

				return True
			except (RequestException, Timeout):
				if retry >= max_retries:
					return False

	def crop_image(self, src_file: Path, dest_file: Path):
		"""è£å‰ªå›¾ç‰‡ä»¥æå–å³ä¾§æŒ‡å®šåŒºåŸŸ

		Args:
			src_file: è¾“å…¥å›¾ç‰‡æ–‡ä»¶è·¯å¾„
			dest_file: è¾“å‡ºå›¾ç‰‡æ–‡ä»¶è·¯å¾„
		"""
		from PIL import Image

		with Image.open(src_file) as source_img:
			width, height = source_img.size

			left = width - 379
			top = 0
			right = width
			bottom = height

			cropped_img = source_img.crop((left, top, right, bottom))
			cropped_img.save(dest_file, format='JPEG')
			source_img.save(src_file, format='JPEG')
#endregion


class DVHelper(MovieScraper):
	"""DVåŠ©æ‰‹ä¸»ç±»ï¼Œåè°ƒå„æ¨¡å—å®Œæˆå½±ç‰‡ä¿¡æ¯è·å–å’Œæ•´ç†å·¥ä½œ"""

	def __init__(self):
		super().__init__()

	def organize_folders(self, root_dir: Path):
		"""æ•´ç†æŒ‡å®šç›®å½•ä¸‹çš„å½±ç‰‡æ–‡ä»¶å¤¹

		Args:
			root_dir: è¦æ•´ç†çš„æ ¹ç›®å½•
		"""
		# æ„å»ºåå‘åˆ«åæ˜ å°„è¡¨ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾å›ºå®šåç§°
		reverse_alias_map = {}
		for fixed_name, aliases in config.actress_alias.items():
			for alias in aliases:
				reverse_alias_map[alias] = fixed_name

		# æ”¶é›†æ‰€æœ‰å­ç›®å½•ä¸­çš„éœ€è¦å¤„ç†çš„æ–‡ä»¶å¤¹
		def collect_folders_recursive(directory: Path):
			for item in directory.iterdir():
				if item.is_dir():
					fixed_name = reverse_alias_map.get(item.name)

					if fixed_name:
						if item.name == fixed_name:
							continue

						folders_to_process.append((item, fixed_name))
					else:
						collect_folders_recursive(item)

		folders_to_process = []
		collect_folders_recursive(root_dir)

		if not folders_to_process:
			logger.info('ğŸš« æœªå‘ç°éœ€è¦æ•´ç†çš„å½±ç‰‡æ–‡ä»¶å¤¹')
			return

		logger.info(f'å‘ç° {len(folders_to_process)} ä¸ªéœ€è¦æ•´ç†çš„å½±ç‰‡æ–‡ä»¶å¤¹')
		for index, (source_folder, target_name) in enumerate(folders_to_process, 1):
			print(f'    {index}.{Path(source_folder).relative_to(root_dir)}')

		# å¤„ç†æ¯ä¸ªéœ€è¦é‡å‘½åçš„ç›®å½•
		for index, (source_folder, target_name) in enumerate(folders_to_process, start=1):
			target_folder = source_folder.parent / target_name

			print()
			logger.info(f'[{index}/{len(folders_to_process)}] ğŸ”„ æ­£åœ¨å¤„ç† {source_folder}...')

			try:
				if not target_folder.exists():
					source_folder.rename(target_folder)
					logger.info(f'å·²å°†æ–‡ä»¶å¤¹é‡å‘½åä¸º: {target_folder}')
				else:
					logger.info(f'ç›®æ ‡ {target_folder} å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆå¹¶æ–‡ä»¶å¤¹...')
					self.__merge_folders(source_folder, target_folder)
					logger.info(f'å·²å®Œæˆä¸ç›®æ ‡æ–‡ä»¶å¤¹ {target_folder} çš„åˆå¹¶')
			except Exception as e:
				logger.error(f'ğŸš« å¤„ç†æ–‡ä»¶å¤¹ {source_folder} æ—¶å‡ºé”™: {str(e)}')

	def __merge_folders(self, source_folder: Path, target_folder: Path):
		"""åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶å¤¹çš„å†…å®¹

		Args:
			source_folder: æºæ–‡ä»¶å¤¹
			target_folder: ç›®æ ‡æ–‡ä»¶å¤¹
		"""
		for item in source_folder.iterdir():
			if item.is_dir():
				target_item = target_folder / item.name
				if target_item.exists() and target_item.is_dir():
					logger.info(f'æ­£åœ¨æ¯”è¾ƒæ–‡ä»¶å¤¹ {item.name}...')
					self.__merge_movie_folders(item, target_item)
				else:
					item.rename(target_item)
					logger.info(f'å·²ç§»åŠ¨å­æ–‡ä»¶å¤¹ {item.name}')
			else:
				target_item = target_folder / item.name
				if not target_item.exists():
					item.rename(target_item)
					logger.info(f'å·²ç§»åŠ¨æ–‡ä»¶ {item.name}')

		try:
			source_folder.rmdir()
			logger.info(f'å·²åˆ é™¤æºæ–‡ä»¶å¤¹ {source_folder}')
		except Exception:
			logger.error(f'ğŸš« æ— æ³•åˆ é™¤æºæ–‡ä»¶å¤¹ {source_folder}')

	def __merge_movie_folders(self, source_folder: Path, target_folder: Path):
		"""åˆå¹¶ä¸¤ä¸ªå½±ç‰‡æ–‡ä»¶å¤¹ï¼Œä¿ç•™è¾ƒå¤§çš„è§†é¢‘æ–‡ä»¶

		Args:
			source_folder: æºå½±ç‰‡æ–‡ä»¶å¤¹
			target_folder: ç›®æ ‡å½±ç‰‡æ–‡ä»¶å¤¹
		"""
		source_movies = {}
		for item in source_folder.iterdir():
			if item.is_file() and any(item.name.lower().endswith(ext) for ext in config.movie_file_extensions):
				source_movies[item.name.lower()] = item

		target_movies = {}
		for item in target_folder.iterdir():
			if item.is_file() and any(item.name.lower().endswith(ext) for ext in config.movie_file_extensions):
				target_movies[item.name.lower()] = item

		for movie_name, source_movie in source_movies.items():
			if movie_name in target_movies:
				target_movie = target_movies[movie_name]
				source_size = source_movie.stat().st_size
				target_size = target_movie.stat().st_size

				if source_size > target_size:
					target_movie.unlink()
					source_movie.rename(target_folder / source_movie.name)
					logger.info(f'ä¿ç•™æºè§†é¢‘å¹¶åˆ é™¤ç›®æ ‡æ–‡ä»¶å¤¹åŒåæ–‡ä»¶ï¼š{source_movie.name}')
				else:
					source_movie.unlink()
					logger.info(f'ä¿ç•™ç›®æ ‡è§†é¢‘å¹¶åˆ é™¤æºæ–‡ä»¶å¤¹åŒåæ–‡ä»¶ï¼š{target_movie.name}')
			else:
				source_movie.rename(target_folder / source_movie.name)
				logger.info(f'å·²ç§»åŠ¨è§†é¢‘æ–‡ä»¶: {source_movie.name}')

		# ç§»åŠ¨æºæ–‡ä»¶å¤¹ä¸­çš„éè§†é¢‘æ–‡ä»¶
		for item in source_folder.iterdir():
			if item.is_file() and not any(item.name.lower().endswith(ext) for ext in config.movie_file_extensions):
				target_item = target_folder / item.name

				if not target_item.exists():
					item.rename(target_item)
				else:
					item.unlink()

		try:
			source_folder.rmdir()
			logger.info(f'å·²åˆ é™¤æºå½±ç‰‡æ–‡ä»¶å¤¹ {source_folder.name}')
		except Exception:
			logger.error(f'ğŸš« æ— æ³•åˆ é™¤æºå½±ç‰‡æ–‡ä»¶å¤¹ {source_folder.name}')

	def analyze_keyword(self, keyword: str):
		"""ä»å·²çŸ¥ä¿¡æ¯ä¸­åˆ†æå¹¶æå–å½±ç‰‡ID

		Args:
			keyword: å·²çŸ¥çš„å½±ç‰‡åç§°æˆ–å…³é”®è¯

		Returns:
			æå–çš„å½±ç‰‡IDï¼Œå¦åˆ™è¿”å›None
		"""
		keyword = config.ignored_movie_pattern.sub('', keyword).upper()

		if 'FC2' in keyword:
			match = config.fc2_movie_pattern.search(keyword)
			if match:
				return f'FC2-{match.group(2)}'
		elif '259LUXU' in keyword:
			match = config._259luxu_movie_pattern.search(keyword)
			if match:
				return f'259LUXU-{match.group(1)}'
		elif '200GANA' in keyword:
			match = config._200gana_movie_pattern.search(keyword)
			if match:
				return f'200GANA-{match.group(1)}'
		elif '300MIUM' in keyword:
			match = config._300mium_movie_pattern.search(keyword)
			if match:
				return f'300MIUM-{match.group(1)}'
		else:
			match = config.normal_movie_pattern.search(keyword)
			if match:
				return match.group(1) + '-' + match.group(2)

			match = config.normal_movie_pattern2.search(keyword)
			if match:
				return match.group(1) + '-' + match.group(2)

	def list_video_files(self, root_dir: Path, max_depth: int=0):
		"""åœ¨æŒ‡å®šç›®å½•ä¸­æœç´¢è§†é¢‘æ–‡ä»¶

		Args:
			root_dir: æ ¹ç›®å½•è·¯å¾„
			max_depth: æœ€å¤§æœç´¢æ·±åº¦ï¼Œ0è¡¨ç¤ºä»…æœç´¢å½“å‰ç›®å½•

		Returns:
			ç¬¦åˆæ¡ä»¶çš„è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
		"""
		found_files = []
		root_depth = len(root_dir.absolute().parts)
		max_depth = max_depth if max_depth >= 0 else 1

		for dirpath, dirnames, filenames in os.walk(root_dir, topdown=True):
			current_depth = len(Path(dirpath).absolute().parts) - root_depth

			if current_depth > max_depth:
				continue

			# æ’é™¤æŒ‡å®šåç§°çš„æ–‡ä»¶å¤¹
			dirnames[:] = [dir_name for dir_name in dirnames if dir_name not in config.exclude_path]

			for filename in filenames:
				if filename.startswith(config.ignored_file_prefix):
					continue

				if any(filename.lower().endswith(ext) for ext in config.movie_file_extensions):
					found_files.append(Path(dirpath) / filename)

		return found_files

	def batch_process(self, keywords: list[str], *, gallery: bool=False, dir_mode: bool=False, root_dir: Path=None):
		"""å¤„ç†å½±ç‰‡çš„ä¿¡æ¯æœç´¢ä¸æ•´ç†

		æ ¹æ®å…³é”®è¯åˆ—è¡¨æˆ–ç›®å½•è·¯å¾„æ‰¹é‡å¤„ç†å½±ç‰‡æ–‡ä»¶ï¼ŒåŒ…æ‹¬æœç´¢å½±ç‰‡ä¿¡æ¯ã€ä¸‹è½½å°é¢å›¾ç‰‡ã€
		å‰§ç…§ã€é¢„å‘Šç‰‡ã€ç”Ÿæˆ NFO æ–‡ä»¶å¹¶æŒ‰æ¼”å‘˜åˆ†ç±»æ•´ç†æ–‡ä»¶ç»“æ„

		Args:
			keywords: æœç´¢å…³é”®è¯åˆ—è¡¨æˆ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
			gallery: æ˜¯å¦ä¸‹è½½å‰§ç…§å’Œé¢„å‘Šç‰‡ï¼Œé»˜è®¤ä¸ºFalse
			dir_mode: æ˜¯å¦ä¸ºç›®å½•æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalse
			root_dir: ç›®å½•æ¨¡å¼ä¸‹çš„æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºNone
		"""

		if dir_mode:
			assert root_dir is not None, 'ç›®å½•æ¨¡å¼ä¸‹å¿…é¡»æä¾›æ ¹ç›®å½•è·¯å¾„'

		failed_movies  = []
		ignored_movies = []

		for index, item in enumerate(keywords, 1):
			keyword = Path(item).name if dir_mode else item

			print()
			logger.info(f'[{index}/{len(keywords)}] ğŸ”„ æ­£åœ¨æœç´¢ {keyword}...')

			movie_id = self.analyze_keyword(keyword)

			if not movie_id:
				logger.warning('ğŸš« æ— æ³•æå–å½±ç‰‡IDï¼Œå¯ä»¥å°è¯•ä¿®æ”¹æ–‡ä»¶ååå†è¯•')
				failed_movies.append(item)
				continue

			tqdm_steps = 6 if dir_mode else 5

			with trange(tqdm_steps, desc=f'å¤„ç† {movie_id}', unit='æ­¥',
						leave=False, ncols=80, bar_format='{l_bar}{bar}|') as step_pbar:
				#region 1. æœç´¢å½±ç‰‡
				step_pbar.set_description(f'æœç´¢å½±ç‰‡')
				response_text = self.fetch_data(f'{config.search_url}{urllib.parse.quote_plus(movie_id)}')
				search_results = MovieParser.parse_search_results(response_text, movie_id)

				if not search_results:
					logger.warning('ğŸš« æœªæ‰¾åˆ°åŒ¹é…çš„å½±ç‰‡')
					failed_movies.append(item)
					continue

				step_pbar.update()
				#endregion

				#region 2. è·å–å½±ç‰‡è¯¦æƒ…
				step_pbar.set_description('è·å–å½±ç‰‡è¯¦æƒ…')
				response_text = self.fetch_data(search_results['detail_url'])
				movie_details = MovieParser.parse_movie_details(response_text)

				if not movie_details:
					logger.warning('ğŸš« æ— æ³•è·å–å½±ç‰‡è¯¦æƒ…')
					failed_movies.append(item)
					continue

				step_pbar.update()

				movie_details.update({
					'detail_url': search_results['detail_url'],
					'title'     : search_results['title'],
					'fanart_url': search_results['fanart_url'],
				})

				movie_info = MovieInfo(movie_details)
				#endregion

				#region 3. æŒ‰æ¼”å‘˜ç»„ç»‡ç›®å½•ç»“æœå¹¶åˆ›å»ºå½±ç‰‡ç›®å½•
				step_pbar.set_description('åˆ›å»ºå½±ç‰‡ç›®å½•')
				actress_count = len(movie_info.actresses)

				if actress_count == 0:
					dir1 = '==æ— åæ¼”å‘˜=='
				elif actress_count == 1:
					dir1 = movie_info.actresses[0]
				else:
					dir1 = '==å¤šæ¼”å‘˜=='

				base_dir = (Path(root_dir) if dir_mode else Path(os.getcwd())) / config.completed_path
				movie_path = base_dir / dir1 / f'[{movie_info.number}]({movie_info.year})'
				movie_path.mkdir(parents=True, exist_ok=True)
				step_pbar.update()
				#endregion

				#region 4. ä¸‹è½½å¹¶å¤„ç†å°é¢å›¾ç‰‡
				step_pbar.set_description('ä¸‹è½½å°é¢å›¾ç‰‡' + 'å’Œå‰§ç…§' if gallery and movie_info.galleries else '')

				if not self.fetch_media(movie_path, config.fanart_image, movie_info.fanart_url, crop=True):
					logger.warning('ğŸš« å°é¢å›¾ç‰‡ä¸‹è½½å¤±è´¥')
					failed_movies.append(item)
					continue

				# ä¸‹è½½å‰§ç…§
				if gallery and movie_info.galleries:
					for i, gallery_url in enumerate(movie_info.galleries):
						_, ext = os.path.splitext(gallery_url.split('?')[0])
						ext = ext.lower() or '.jpg'
						self.fetch_media(movie_path, f'gallery_{i:02d}{ext}', gallery_url)

				# ä¸‹è½½é¢„å‘Šç‰‡
				if gallery and movie_info.trailer_url:
					_, ext = os.path.splitext(movie_info.trailer_url.split('?')[0])
					ext = ext.lower() or '.mp4'
					self.fetch_media(movie_path, f'{movie_info.number}_trailer{ext}', movie_info.trailer_url)

				step_pbar.update()
				#endregion

				#region 5. ç”ŸæˆNFOæ–‡ä»¶
				step_pbar.set_description(f'ç”Ÿæˆ NFO æ–‡ä»¶')
				nfo = NFOGenerator(movie_info)
				nfo.save(f'{movie_path}/{movie_info.number}.nfo')
				step_pbar.update()
				#endregion

				#region 6. ç§»åŠ¨å½±ç‰‡æºæ–‡ä»¶åˆ°æ•´ç†åçš„æ–‡ä»¶å¤¹å¹¶æ”¹å
				if dir_mode:
					step_pbar.set_description(f'ç§»åŠ¨å½±ç‰‡æ–‡ä»¶')

					old_path = Path(item)
					new_path = movie_path / old_path.with_stem(movie_info.number.upper())\
													.with_suffix(old_path.suffix.lower()).name

					if new_path.exists():
						old_file_size = old_path.stat().st_size
						new_file_size = new_path.stat().st_size

						if old_file_size <= new_file_size:
							ignored_path = old_path.parent / f'{config.ignored_file_prefix}{old_path.name}'
							old_path.rename(ignored_path)
							ignored_movies.append(old_path)
						else:
							old_path.rename(new_path)
					else:
						old_path.rename(new_path)

					step_pbar.update()

				logger.info(f'âœ… å½±ç‰‡ç›¸å…³æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{movie_path}')
				#endregion

		print()
		logger.info(f'æœç´¢å®Œæˆï¼Œå…±æœç´¢æ•´ç† {len(keywords)} éƒ¨å½±ç‰‡ï¼Œå…¶ä¸­ {len(failed_movies)} éƒ¨å½±ç‰‡è·å–ä¿¡æ¯å¤±è´¥')

		if failed_movies:
			print('è·å–ä¿¡æ¯å¤±è´¥çš„å½±ç‰‡ï¼š')
			for index, movie in enumerate(failed_movies, 1):
				print(f'    {index}.{Path(movie).relative_to(root_dir) if dir_mode else movie}')

		if ignored_movies:
			print('è¢«å¿½ç•¥çš„å½±ç‰‡æ–‡ä»¶ï¼š')
			for index, movie in enumerate(ignored_movies, 1):
				print(f'    {index}.{Path(movie).relative_to(root_dir) if dir_mode else movie}')


def get_logger():
	import logging

	logger = logging.getLogger(__name__)
	logger.setLevel(logging.INFO)

	# ç§»é™¤ç°æœ‰çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤
	if logger.hasHandlers():
		logger.handlers = []

	# æ–‡ä»¶å¤„ç†å™¨ - è¯¦ç»†æ ¼å¼ç”¨äºæŒä¹…åŒ–
	file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
	file_handler = logging.FileHandler('dvhelper.log', encoding='utf-8')
	file_handler.setFormatter(file_formatter)

	# æ§åˆ¶å°å¤„ç†å™¨ - ç®€æ´æ ¼å¼ç”¨äºæ§åˆ¶å°æ˜¾ç¤º
	console_formatter = logging.Formatter('%(message)s')
	console_handler = logging.StreamHandler(sys.stdout)
	console_handler.setFormatter(console_formatter)
	console_handler.stream = TqdmOut

	# æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—å™¨
	logger.addHandler(file_handler)
	logger.addHandler(console_handler)

	return logger

def lazy_import():
	global logger
	global requests, RequestException, Timeout
	global tqdm, trange

	import requests
	from requests.exceptions import RequestException, Timeout
	from tqdm import tqdm, trange

	logger = get_logger()

def main():
	"""åº”ç”¨ç¨‹åºå…¥å£ç‚¹"""

	global config
	config = Config()

	# å½“ç¨‹åºä½œä¸ºexeè¿è¡Œæ—¶ï¼Œä½¿ç”¨sys.executableè·å–æ­£ç¡®çš„è¿è¡Œç›®å½•
	if getattr(sys, 'frozen', False):
		# è·å–exeæ‰€åœ¨ç›®å½•
		current_dir = Path(sys.executable).parent
		config.actress_alias_file = current_dir.joinpath('actress_alias.json')
		config.cookies_file = current_dir.joinpath('cookies.json')

	parser = HelpOnErrorParser(
		description=config.description,
		usage='%(prog)s [options] keywords_or_path',
		formatter_class=RawTextRichHelpFormatter,
		# epilog=config.epilog
	)

	parser.add_argument('-v', '--version', action='version', version=f'[argparse.prog]DV Helper[/] (version [i]{__version__}[/])')
	parser.add_argument('keywords_or_path', type=str, help=config.keywords_help)
	parser.add_argument('-d', '--depth', type=int, default=0, help=config.depth_help)
	parser.add_argument('-g', '--gallery', action='store_true', help=config.gallery_help)
	parser.add_argument('-l', '--login', action='store_true', help=config.login_help)
	parser.add_argument('-o', '--organize', action='store_true', help='æ•´ç†å¹¶é‡å‘½åæŒ‡å®šç›®å½•ä¸‹çš„å½±ç‰‡æ–‡ä»¶å¤¹')

	if len(sys.argv) == 1:
		parser.print_help()
		sys.exit(0)

	args, _ = parser.parse_known_args()

	lazy_import()

	dv_helper = DVHelper()
	keywords_or_path: str = args.keywords_or_path

	if args.login:
		if dv_helper.perform_login() is None:
			sys.exit(0)

	dv_helper.initialize_session()

	if config.actress_alias_file.exists():
		with open(config.actress_alias_file, 'r', encoding='utf-8') as file:
			config.actress_alias = json.load(file)

	if Path(keywords_or_path).absolute().is_dir():
		root_dir = Path(keywords_or_path)

		if args.organize:
			if not config.actress_alias:
				logger.warning('ğŸš« actress_alias.json æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œæ•´ç†æ“ä½œ')
			else:
				dv_helper.organize_folders(root_dir)
			return

		found_files = dv_helper.list_video_files(root_dir, max_depth=args.depth)

		if found_files:
			logger.info(f'å‘ç° {len(found_files)} ä¸ªå½±ç‰‡æ–‡ä»¶')
			for index, file_path in enumerate(found_files, 1):
				print(f'    {index}.{Path(file_path).relative_to(root_dir)}')

			dv_helper.batch_process(found_files, gallery=args.gallery, dir_mode=True, root_dir=root_dir)
		else:
			logger.info(f"åœ¨ {root_dir} {'åŠå…¶å­ç›®å½•' if args.depth > 0 else ''}ä¸­æœªå‘ç°å½±ç‰‡æ–‡ä»¶")
	else:
		keywords = [keyword.strip() for keyword in keywords_or_path.split(',')]
		logger.info(f'å¾…å¤„ç† {len(keywords)} ä¸ªå½±ç‰‡å…³é”®è¯')

		for index, keyword in enumerate(keywords, 1):
			print(f'    {index}.{keyword}')

		dv_helper.batch_process(keywords, gallery=args.gallery)


if __name__ == '__main__':
	main()
